<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>rl_agent.utils.reward API documentation</title>
<meta name="description" content="Reward Calculator for DRL" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rl_agent.utils.reward</code></h1>
</header>
<section id="section-intro">
<p>Reward Calculator for DRL</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Reward Calculator for DRL&#34;&#34;&#34;
import numpy as np
import scipy.spatial

from geometry_msgs.msg import Pose2D
from typing import Dict, Tuple, Union


class RewardCalculator:
    def __init__(
        self,
        robot_radius: float,
        safe_dist: float,
        goal_radius: float,
        rule: str = &#34;rule_00&#34;,
        extended_eval: bool = False,
    ):
        &#34;&#34;&#34;A facotry class for reward calculation. Holds various reward functions.

        An overview of the reward functions can be found under:
        https://github.com/ignc-research/arena-rosnav/blob/local_planner_subgoalmode/docs/DRL-Training.md#reward-functions

        Possible reward functions: &#34;_rule_00_&#34;, &#34;_rule_01_&#34;, &#34;_rule_02_&#34;, &#34;_rule_03_&#34;, &#34;_rule_04_&#34;

        Args:
            robot_radius (float): Robots&#39; radius in meters.
            safe_dist (float): Robots&#39; safe distance in meters.
            goal_radius (float): Radius of the goal.
            rule (str, optional): The desired reward function name. Defaults to &#34;rule_00&#34;.
            extended_eval (bool, optional): Extended evaluation mode. Defaults to False.
        &#34;&#34;&#34;
        self.curr_reward = 0
        # additional info will be stored here and be returned alonge with reward.
        self.info = {}
        self.robot_radius = robot_radius
        self.goal_radius = goal_radius
        self.last_goal_dist = None
        self.last_dist_to_path = None
        self.last_action = None
        self.safe_dist = robot_radius + safe_dist
        self._extended_eval = extended_eval

        self.kdtree = None

        self._cal_funcs = {
            &#34;rule_00&#34;: RewardCalculator._cal_reward_rule_00,
            &#34;rule_01&#34;: RewardCalculator._cal_reward_rule_01,
            &#34;rule_02&#34;: RewardCalculator._cal_reward_rule_02,
            &#34;rule_03&#34;: RewardCalculator._cal_reward_rule_03,
            &#34;rule_04&#34;: RewardCalculator._cal_reward_rule_04,
        }
        self.cal_func = self._cal_funcs[rule]

    def reset(self) -&gt; None:
        &#34;&#34;&#34;Resets variables related to the episode.&#34;&#34;&#34;
        self.last_goal_dist = None
        self.last_dist_to_path = None
        self.last_action = None
        self.kdtree = None

    def _reset(self) -&gt; None:
        &#34;&#34;&#34;Resets variables related to current step.&#34;&#34;&#34;
        self.curr_reward = 0
        self.info = {}

    def get_reward(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ) -&gt; Tuple[float, Dict[str, Union[str, int, bool]]]:
        &#34;&#34;&#34;Returns reward and info to the gym environment.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).

        Returns:
            Tuple[float, Dict[str, Union[str, int, bool]]]: Tuple of calculated rewards for the current step, \
                and the reward information dictionary.
        &#34;&#34;&#34;
        self._reset()
        self.cal_func(self, laser_scan, goal_in_robot_frame, *args, **kwargs)
        return self.curr_reward, self.info

    def _cal_reward_rule_00(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_00_&#39;

        Description:
            &#34;rule_00&#34; incorporates the most instinctive characteristics for learning navigation into its \
            reward calculation. The reward function is made up of only 4 summands, namely the success \
            reward, the collision reward, the danger reward and the progress reward. Similar reward functions \
            were utilized in numerous research projects and produced promising results. Thus, this \
            rule is chosen to be the basis for further experiments with extended versions of it. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_goal_reached(goal_in_robot_frame)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_01(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_01_&#39;
        
        Description:
            This reward function extends &#34;rule 00&#34; by adding a penalty factor that affects the current \
            reward like an abstract fuel consumption factor. In principle, a certain penalty is applied \
            for each action taken depending on the velocity and thus imposes a severer punishment for \
            dissipated driving.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_distance_traveled(
            kwargs[&#34;action&#34;], consumption_factor=0.0075
        )
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_02(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_02_&#39;
        
        Description:
            Previous reward functions required only basic information from the simulation. For this rule, \
            which builds on the reward function &#34;rule 01&#34;, we introduced the assessment of the progress \
            regarding the global plan. The additional summand essentially rewards the agent for following \
            the global plan. It was implemented in order to test the effect of including the global plan in \
            the reward calculation. \
            Since &#34;rule 02&#34; shares almost the same reward function composition as &#34;rule 01&#34;, similar performance \
            was expected to some extent. The desired behavior for this agent was to learn faster and \
            to drive more goal-oriented than the agent of &#34;rule 01&#34;, as this rule was provided the global plan. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_distance_traveled(
            kwargs[&#34;action&#34;], consumption_factor=0.0075
        )
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;]
        )
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_03(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_03_&#39;

        Description:
            The base of this rule is made up of summands from &#34;rule 00&#34;. The two extra factors were \
            introduced in order to further leverage the global plan information for reward generation. \
            One that rewards the following of the global path and one for valuing the agents’ action - \
            positively, when it approaches the global plan - negatively when the robot distances itself \
            from the path. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data. \
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate). \
        &#34;&#34;&#34;
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;], kwargs[&#34;action&#34;]
        )
        if laser_scan.min() &gt; self.safe_dist:
            self._reward_distance_global_plan(
                kwargs[&#34;global_plan&#34;],
                kwargs[&#34;robot_pose&#34;],
                reward_factor=0.2,
                penalty_factor=0.3,
            )
        else:
            self.last_dist_to_path = None
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_04(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_04_&#39;

        Description:
            This reward function extends &#34;rule 03&#34; with an additional term that punishes the agent for \
            abruptly changing the direction. Previous test runs, conducted right after the implementation, \
            evidenced that although the agent performed well on different tasks, the robot tended to drive \
            in tail motion. It was aimed to adjust this behavior by including this additional penalty term. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_abrupt_direction_change(kwargs[&#34;action&#34;])
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;], kwargs[&#34;action&#34;]
        )
        if laser_scan.min() &gt; self.safe_dist:
            self._reward_distance_global_plan(
                kwargs[&#34;global_plan&#34;],
                kwargs[&#34;robot_pose&#34;],
                reward_factor=0.2,
                penalty_factor=0.3,
            )
        else:
            self.last_dist_to_path = None
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _reward_goal_reached(
        self, goal_in_robot_frame: Tuple[float, float], reward: float = 15
    ):
        &#34;&#34;&#34;Reward for reaching the goal.

        Args:
            goal_in_robot_frame (Tuple[float, float], optional): Position (rho, theta) of the goal in the robot frame (polar coordinate).
            reward (float, optional): Reward amount for reaching the goal. Defaults to 15.
        &#34;&#34;&#34;
        if goal_in_robot_frame[0] &lt; self.goal_radius:
            self.curr_reward = reward
            self.info[&#34;is_done&#34;] = True
            self.info[&#34;done_reason&#34;] = 2
            self.info[&#34;is_success&#34;] = 1
        else:
            self.info[&#34;is_done&#34;] = False

    def _reward_goal_approached(
        self,
        goal_in_robot_frame=Tuple[float, float],
        reward_factor: float = 0.3,
        penalty_factor: float = 0.5,
    ):
        &#34;&#34;&#34;Reward for approaching the goal.

        Args:
            goal_in_robot_frame ([type], optional): Position (rho, theta) of the goal in the robot frame (polar coordinate). Defaults to Tuple[float, float].
            reward_factor (float, optional): Factor to be multiplied when the difference between current distance to goal and the previous one is positive. \
                Defaults to 0.3.
            penalty_factor (float, optional): Factor to be multiplied when the difference between current distance to goal and the previous one is negative. Defaults to 0.5.
        &#34;&#34;&#34;
        if self.last_goal_dist is not None:
            # goal_in_robot_frame : [rho, theta]

            # higher negative weight when moving away from goal
            # (to avoid driving unnecessary circles when train in contin. action space)
            if (self.last_goal_dist - goal_in_robot_frame[0]) &gt; 0:
                w = reward_factor
            else:
                w = penalty_factor
            reward = w * (self.last_goal_dist - goal_in_robot_frame[0])

            # print(&#34;reward_goal_approached:  {}&#34;.format(reward))
            self.curr_reward += reward
        self.last_goal_dist = goal_in_robot_frame[0]

    def _reward_collision(self, laser_scan: np.ndarray, punishment: float = 10):
        &#34;&#34;&#34;Reward for colliding with an obstacle.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            punishment (float, optional): Punishment amount for collisions. Defaults to 10.
        &#34;&#34;&#34;
        if laser_scan.min() &lt;= self.robot_radius:
            self.curr_reward -= punishment

            if not self._extended_eval:
                self.info[&#34;is_done&#34;] = True
                self.info[&#34;done_reason&#34;] = 1
                self.info[&#34;is_success&#34;] = 0
            else:
                self.info[&#34;crash&#34;] = True

    def _reward_safe_dist(
        self, laser_scan: np.ndarray, punishment: float = 0.15
    ):
        &#34;&#34;&#34;Reward for undercutting safe distance.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            punishment (float, optional): Punishment amount. Could be applied in consecutive timesteps. \
                Defaults to 0.15.
        &#34;&#34;&#34;
        if laser_scan.min() &lt; self.safe_dist:
            self.curr_reward -= punishment

            if self._extended_eval:
                self.info[&#34;safe_dist&#34;] = True

    def _reward_not_moving(
        self, action: np.ndarray = None, punishment: float = 0.01
    ):
        &#34;&#34;&#34;Reward for not moving. 

        Args:
            action (np.ndarray, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            punishment (float, optional): Punishment for not moving. Defaults to 0.01.
            
        Note:
            Only applies half of the punishment amount when angular velocity is larger than zero.
        &#34;&#34;&#34;
        if action is not None and action[0] == 0.0:
            self.curr_reward -= (
                punishment if action[1] == 0.0 else punishment / 2
            )

    def _reward_distance_traveled(
        self,
        action: np.array = None,
        punishment: float = 0.01,
        consumption_factor: float = 0.005,
    ):
        &#34;&#34;&#34;Reward for driving a certain distance. Supposed to represent &#34;fuel consumption&#34;.

        Args:
            action (np.array, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            punishment (float, optional): Punishment when action can&#39;t be retrieved. Defaults to 0.01.
            consumption_factor (float, optional): Factor for the weighted velocity punishment. Defaults to 0.005.
        &#34;&#34;&#34;
        if action is None:
            self.curr_reward -= punishment
        else:
            lin_vel = action[0]
            ang_vel = action[1]
            reward = (lin_vel + (ang_vel * 0.001)) * consumption_factor
        self.curr_reward -= reward

    def _reward_distance_global_plan(
        self,
        global_plan: np.array,
        robot_pose: Pose2D,
        reward_factor: float = 0.1,
        penalty_factor: float = 0.15,
    ):
        &#34;&#34;&#34;Reward for approaching/veering away the global plan.

        Description:
            Weighted difference between prior distance to global plan and current distance to global plan.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.
            reward_factor (float, optional): Factor to be multiplied when the difference between current \
                distance to global plan and the previous one is positive. Defaults to 0.1.
            penalty_factor (float, optional): Factor to be multiplied when the difference between current \
                distance to global plan and the previous one is negative. Defaults to 0.15.
        &#34;&#34;&#34;
        if global_plan is not None and len(global_plan) != 0:
            curr_dist_to_path, idx = self.get_min_dist2global_kdtree(
                global_plan, robot_pose
            )

            if self.last_dist_to_path is not None:
                if curr_dist_to_path &lt; self.last_dist_to_path:
                    w = reward_factor
                else:
                    w = penalty_factor

                self.curr_reward += w * (
                    self.last_dist_to_path - curr_dist_to_path
                )
            self.last_dist_to_path = curr_dist_to_path

    def _reward_following_global_plan(
        self,
        global_plan: np.array,
        robot_pose: Pose2D,
        action: np.array = None,
        dist_to_path: float = 0.5,
    ):
        &#34;&#34;&#34;Reward for travelling along the global plan.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.
            action (np.array, optional): action (np.ndarray, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            dist_to_path (float, optional): Minimum distance to the global path. Defaults to 0.5.
        &#34;&#34;&#34;
        if (
            global_plan is not None
            and len(global_plan) != 0
            and action is not None
        ):
            curr_dist_to_path, idx = self.get_min_dist2global_kdtree(
                global_plan, robot_pose
            )

            if curr_dist_to_path &lt;= dist_to_path:
                self.curr_reward += 0.1 * action[0]

    def get_min_dist2global_kdtree(
        self, global_plan: np.array, robot_pose: Pose2D
    ) -&gt; Tuple[float, int]:
        &#34;&#34;&#34;Calculates minimal distance to global plan using kd-tree-search.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.

        Returns:
            Tuple[float, int]: Distance to the closes pose and index of the closes pose.
        &#34;&#34;&#34;
        if self.kdtree is None:
            self.kdtree = scipy.spatial.cKDTree(global_plan)

        dist, index = self.kdtree.query([robot_pose.x, robot_pose.y])
        return dist, index

    def _reward_abrupt_direction_change(self, action: np.array = None):
        &#34;&#34;&#34;Applies a penalty when an abrupt change of direction occured.

        Args:
            action (np.array, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
        &#34;&#34;&#34;
        if self.last_action is not None:
            curr_ang_vel = action[1]
            last_ang_vel = self.last_action[1]

            vel_diff = abs(curr_ang_vel - last_ang_vel)
            self.curr_reward -= (vel_diff ** 4) / 2500
        self.last_action = action</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rl_agent.utils.reward.RewardCalculator"><code class="flex name class">
<span>class <span class="ident">RewardCalculator</span></span>
<span>(</span><span>robot_radius: float, safe_dist: float, goal_radius: float, rule: str = 'rule_00', extended_eval: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>A facotry class for reward calculation. Holds various reward functions.</p>
<p>An overview of the reward functions can be found under:
<a href="https://github.com/ignc-research/arena-rosnav/blob/local_planner_subgoalmode/docs/DRL-Training.md#reward-functions">https://github.com/ignc-research/arena-rosnav/blob/local_planner_subgoalmode/docs/DRL-Training.md#reward-functions</a></p>
<p>Possible reward functions: "<em>rule_00</em>", "<em>rule_01</em>", "<em>rule_02</em>", "<em>rule_03</em>", "<em>rule_04</em>"</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>robot_radius</code></strong> :&ensp;<code>float</code></dt>
<dd>Robots' radius in meters.</dd>
<dt><strong><code>safe_dist</code></strong> :&ensp;<code>float</code></dt>
<dd>Robots' safe distance in meters.</dd>
<dt><strong><code>goal_radius</code></strong> :&ensp;<code>float</code></dt>
<dd>Radius of the goal.</dd>
<dt><strong><code>rule</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The desired reward function name. Defaults to "rule_00".</dd>
<dt><strong><code>extended_eval</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Extended evaluation mode. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RewardCalculator:
    def __init__(
        self,
        robot_radius: float,
        safe_dist: float,
        goal_radius: float,
        rule: str = &#34;rule_00&#34;,
        extended_eval: bool = False,
    ):
        &#34;&#34;&#34;A facotry class for reward calculation. Holds various reward functions.

        An overview of the reward functions can be found under:
        https://github.com/ignc-research/arena-rosnav/blob/local_planner_subgoalmode/docs/DRL-Training.md#reward-functions

        Possible reward functions: &#34;_rule_00_&#34;, &#34;_rule_01_&#34;, &#34;_rule_02_&#34;, &#34;_rule_03_&#34;, &#34;_rule_04_&#34;

        Args:
            robot_radius (float): Robots&#39; radius in meters.
            safe_dist (float): Robots&#39; safe distance in meters.
            goal_radius (float): Radius of the goal.
            rule (str, optional): The desired reward function name. Defaults to &#34;rule_00&#34;.
            extended_eval (bool, optional): Extended evaluation mode. Defaults to False.
        &#34;&#34;&#34;
        self.curr_reward = 0
        # additional info will be stored here and be returned alonge with reward.
        self.info = {}
        self.robot_radius = robot_radius
        self.goal_radius = goal_radius
        self.last_goal_dist = None
        self.last_dist_to_path = None
        self.last_action = None
        self.safe_dist = robot_radius + safe_dist
        self._extended_eval = extended_eval

        self.kdtree = None

        self._cal_funcs = {
            &#34;rule_00&#34;: RewardCalculator._cal_reward_rule_00,
            &#34;rule_01&#34;: RewardCalculator._cal_reward_rule_01,
            &#34;rule_02&#34;: RewardCalculator._cal_reward_rule_02,
            &#34;rule_03&#34;: RewardCalculator._cal_reward_rule_03,
            &#34;rule_04&#34;: RewardCalculator._cal_reward_rule_04,
        }
        self.cal_func = self._cal_funcs[rule]

    def reset(self) -&gt; None:
        &#34;&#34;&#34;Resets variables related to the episode.&#34;&#34;&#34;
        self.last_goal_dist = None
        self.last_dist_to_path = None
        self.last_action = None
        self.kdtree = None

    def _reset(self) -&gt; None:
        &#34;&#34;&#34;Resets variables related to current step.&#34;&#34;&#34;
        self.curr_reward = 0
        self.info = {}

    def get_reward(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ) -&gt; Tuple[float, Dict[str, Union[str, int, bool]]]:
        &#34;&#34;&#34;Returns reward and info to the gym environment.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).

        Returns:
            Tuple[float, Dict[str, Union[str, int, bool]]]: Tuple of calculated rewards for the current step, \
                and the reward information dictionary.
        &#34;&#34;&#34;
        self._reset()
        self.cal_func(self, laser_scan, goal_in_robot_frame, *args, **kwargs)
        return self.curr_reward, self.info

    def _cal_reward_rule_00(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_00_&#39;

        Description:
            &#34;rule_00&#34; incorporates the most instinctive characteristics for learning navigation into its \
            reward calculation. The reward function is made up of only 4 summands, namely the success \
            reward, the collision reward, the danger reward and the progress reward. Similar reward functions \
            were utilized in numerous research projects and produced promising results. Thus, this \
            rule is chosen to be the basis for further experiments with extended versions of it. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_goal_reached(goal_in_robot_frame)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_01(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_01_&#39;
        
        Description:
            This reward function extends &#34;rule 00&#34; by adding a penalty factor that affects the current \
            reward like an abstract fuel consumption factor. In principle, a certain penalty is applied \
            for each action taken depending on the velocity and thus imposes a severer punishment for \
            dissipated driving.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_distance_traveled(
            kwargs[&#34;action&#34;], consumption_factor=0.0075
        )
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_02(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_02_&#39;
        
        Description:
            Previous reward functions required only basic information from the simulation. For this rule, \
            which builds on the reward function &#34;rule 01&#34;, we introduced the assessment of the progress \
            regarding the global plan. The additional summand essentially rewards the agent for following \
            the global plan. It was implemented in order to test the effect of including the global plan in \
            the reward calculation. \
            Since &#34;rule 02&#34; shares almost the same reward function composition as &#34;rule 01&#34;, similar performance \
            was expected to some extent. The desired behavior for this agent was to learn faster and \
            to drive more goal-oriented than the agent of &#34;rule 01&#34;, as this rule was provided the global plan. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_distance_traveled(
            kwargs[&#34;action&#34;], consumption_factor=0.0075
        )
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;]
        )
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_03(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_03_&#39;

        Description:
            The base of this rule is made up of summands from &#34;rule 00&#34;. The two extra factors were \
            introduced in order to further leverage the global plan information for reward generation. \
            One that rewards the following of the global path and one for valuing the agents’ action - \
            positively, when it approaches the global plan - negatively when the robot distances itself \
            from the path. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data. \
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate). \
        &#34;&#34;&#34;
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;], kwargs[&#34;action&#34;]
        )
        if laser_scan.min() &gt; self.safe_dist:
            self._reward_distance_global_plan(
                kwargs[&#34;global_plan&#34;],
                kwargs[&#34;robot_pose&#34;],
                reward_factor=0.2,
                penalty_factor=0.3,
            )
        else:
            self.last_dist_to_path = None
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _cal_reward_rule_04(
        self,
        laser_scan: np.ndarray,
        goal_in_robot_frame: Tuple[float, float],
        *args,
        **kwargs
    ):
        &#34;&#34;&#34;Reward function: &#39;_rule\_04_&#39;

        Description:
            This reward function extends &#34;rule 03&#34; with an additional term that punishes the agent for \
            abruptly changing the direction. Previous test runs, conducted right after the implementation, \
            evidenced that although the agent performed well on different tasks, the robot tended to drive \
            in tail motion. It was aimed to adjust this behavior by including this additional penalty term. \

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).
        &#34;&#34;&#34;
        self._reward_abrupt_direction_change(kwargs[&#34;action&#34;])
        self._reward_following_global_plan(
            kwargs[&#34;global_plan&#34;], kwargs[&#34;robot_pose&#34;], kwargs[&#34;action&#34;]
        )
        if laser_scan.min() &gt; self.safe_dist:
            self._reward_distance_global_plan(
                kwargs[&#34;global_plan&#34;],
                kwargs[&#34;robot_pose&#34;],
                reward_factor=0.2,
                penalty_factor=0.3,
            )
        else:
            self.last_dist_to_path = None
        self._reward_goal_reached(goal_in_robot_frame, reward=15)
        self._reward_safe_dist(laser_scan, punishment=0.25)
        self._reward_collision(laser_scan, punishment=10)
        self._reward_goal_approached(
            goal_in_robot_frame, reward_factor=0.3, penalty_factor=0.4
        )

    def _reward_goal_reached(
        self, goal_in_robot_frame: Tuple[float, float], reward: float = 15
    ):
        &#34;&#34;&#34;Reward for reaching the goal.

        Args:
            goal_in_robot_frame (Tuple[float, float], optional): Position (rho, theta) of the goal in the robot frame (polar coordinate).
            reward (float, optional): Reward amount for reaching the goal. Defaults to 15.
        &#34;&#34;&#34;
        if goal_in_robot_frame[0] &lt; self.goal_radius:
            self.curr_reward = reward
            self.info[&#34;is_done&#34;] = True
            self.info[&#34;done_reason&#34;] = 2
            self.info[&#34;is_success&#34;] = 1
        else:
            self.info[&#34;is_done&#34;] = False

    def _reward_goal_approached(
        self,
        goal_in_robot_frame=Tuple[float, float],
        reward_factor: float = 0.3,
        penalty_factor: float = 0.5,
    ):
        &#34;&#34;&#34;Reward for approaching the goal.

        Args:
            goal_in_robot_frame ([type], optional): Position (rho, theta) of the goal in the robot frame (polar coordinate). Defaults to Tuple[float, float].
            reward_factor (float, optional): Factor to be multiplied when the difference between current distance to goal and the previous one is positive. \
                Defaults to 0.3.
            penalty_factor (float, optional): Factor to be multiplied when the difference between current distance to goal and the previous one is negative. Defaults to 0.5.
        &#34;&#34;&#34;
        if self.last_goal_dist is not None:
            # goal_in_robot_frame : [rho, theta]

            # higher negative weight when moving away from goal
            # (to avoid driving unnecessary circles when train in contin. action space)
            if (self.last_goal_dist - goal_in_robot_frame[0]) &gt; 0:
                w = reward_factor
            else:
                w = penalty_factor
            reward = w * (self.last_goal_dist - goal_in_robot_frame[0])

            # print(&#34;reward_goal_approached:  {}&#34;.format(reward))
            self.curr_reward += reward
        self.last_goal_dist = goal_in_robot_frame[0]

    def _reward_collision(self, laser_scan: np.ndarray, punishment: float = 10):
        &#34;&#34;&#34;Reward for colliding with an obstacle.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            punishment (float, optional): Punishment amount for collisions. Defaults to 10.
        &#34;&#34;&#34;
        if laser_scan.min() &lt;= self.robot_radius:
            self.curr_reward -= punishment

            if not self._extended_eval:
                self.info[&#34;is_done&#34;] = True
                self.info[&#34;done_reason&#34;] = 1
                self.info[&#34;is_success&#34;] = 0
            else:
                self.info[&#34;crash&#34;] = True

    def _reward_safe_dist(
        self, laser_scan: np.ndarray, punishment: float = 0.15
    ):
        &#34;&#34;&#34;Reward for undercutting safe distance.

        Args:
            laser_scan (np.ndarray): 2D laser scan data.
            punishment (float, optional): Punishment amount. Could be applied in consecutive timesteps. \
                Defaults to 0.15.
        &#34;&#34;&#34;
        if laser_scan.min() &lt; self.safe_dist:
            self.curr_reward -= punishment

            if self._extended_eval:
                self.info[&#34;safe_dist&#34;] = True

    def _reward_not_moving(
        self, action: np.ndarray = None, punishment: float = 0.01
    ):
        &#34;&#34;&#34;Reward for not moving. 

        Args:
            action (np.ndarray, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            punishment (float, optional): Punishment for not moving. Defaults to 0.01.
            
        Note:
            Only applies half of the punishment amount when angular velocity is larger than zero.
        &#34;&#34;&#34;
        if action is not None and action[0] == 0.0:
            self.curr_reward -= (
                punishment if action[1] == 0.0 else punishment / 2
            )

    def _reward_distance_traveled(
        self,
        action: np.array = None,
        punishment: float = 0.01,
        consumption_factor: float = 0.005,
    ):
        &#34;&#34;&#34;Reward for driving a certain distance. Supposed to represent &#34;fuel consumption&#34;.

        Args:
            action (np.array, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            punishment (float, optional): Punishment when action can&#39;t be retrieved. Defaults to 0.01.
            consumption_factor (float, optional): Factor for the weighted velocity punishment. Defaults to 0.005.
        &#34;&#34;&#34;
        if action is None:
            self.curr_reward -= punishment
        else:
            lin_vel = action[0]
            ang_vel = action[1]
            reward = (lin_vel + (ang_vel * 0.001)) * consumption_factor
        self.curr_reward -= reward

    def _reward_distance_global_plan(
        self,
        global_plan: np.array,
        robot_pose: Pose2D,
        reward_factor: float = 0.1,
        penalty_factor: float = 0.15,
    ):
        &#34;&#34;&#34;Reward for approaching/veering away the global plan.

        Description:
            Weighted difference between prior distance to global plan and current distance to global plan.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.
            reward_factor (float, optional): Factor to be multiplied when the difference between current \
                distance to global plan and the previous one is positive. Defaults to 0.1.
            penalty_factor (float, optional): Factor to be multiplied when the difference between current \
                distance to global plan and the previous one is negative. Defaults to 0.15.
        &#34;&#34;&#34;
        if global_plan is not None and len(global_plan) != 0:
            curr_dist_to_path, idx = self.get_min_dist2global_kdtree(
                global_plan, robot_pose
            )

            if self.last_dist_to_path is not None:
                if curr_dist_to_path &lt; self.last_dist_to_path:
                    w = reward_factor
                else:
                    w = penalty_factor

                self.curr_reward += w * (
                    self.last_dist_to_path - curr_dist_to_path
                )
            self.last_dist_to_path = curr_dist_to_path

    def _reward_following_global_plan(
        self,
        global_plan: np.array,
        robot_pose: Pose2D,
        action: np.array = None,
        dist_to_path: float = 0.5,
    ):
        &#34;&#34;&#34;Reward for travelling along the global plan.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.
            action (np.array, optional): action (np.ndarray, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
            dist_to_path (float, optional): Minimum distance to the global path. Defaults to 0.5.
        &#34;&#34;&#34;
        if (
            global_plan is not None
            and len(global_plan) != 0
            and action is not None
        ):
            curr_dist_to_path, idx = self.get_min_dist2global_kdtree(
                global_plan, robot_pose
            )

            if curr_dist_to_path &lt;= dist_to_path:
                self.curr_reward += 0.1 * action[0]

    def get_min_dist2global_kdtree(
        self, global_plan: np.array, robot_pose: Pose2D
    ) -&gt; Tuple[float, int]:
        &#34;&#34;&#34;Calculates minimal distance to global plan using kd-tree-search.

        Args:
            global_plan (np.array): Array containing 2D poses.
            robot_pose (Pose2D): Robot position.

        Returns:
            Tuple[float, int]: Distance to the closes pose and index of the closes pose.
        &#34;&#34;&#34;
        if self.kdtree is None:
            self.kdtree = scipy.spatial.cKDTree(global_plan)

        dist, index = self.kdtree.query([robot_pose.x, robot_pose.y])
        return dist, index

    def _reward_abrupt_direction_change(self, action: np.array = None):
        &#34;&#34;&#34;Applies a penalty when an abrupt change of direction occured.

        Args:
            action (np.array, optional): Array of shape (2,). First entry, linear velocity. \
                Second entry, angular velocity. Defaults to None.
        &#34;&#34;&#34;
        if self.last_action is not None:
            curr_ang_vel = action[1]
            last_ang_vel = self.last_action[1]

            vel_diff = abs(curr_ang_vel - last_ang_vel)
            self.curr_reward -= (vel_diff ** 4) / 2500
        self.last_action = action</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rl_agent.utils.reward.RewardCalculator.get_min_dist2global_kdtree"><code class="name flex">
<span>def <span class="ident">get_min_dist2global_kdtree</span></span>(<span>self, global_plan: <built-in function array>, robot_pose: geometry_msgs.msg._Pose2D.Pose2D) ‑> Tuple[float, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates minimal distance to global plan using kd-tree-search.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>global_plan</code></strong> :&ensp;<code>np.array</code></dt>
<dd>Array containing 2D poses.</dd>
<dt><strong><code>robot_pose</code></strong> :&ensp;<code>Pose2D</code></dt>
<dd>Robot position.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, int]</code></dt>
<dd>Distance to the closes pose and index of the closes pose.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_min_dist2global_kdtree(
    self, global_plan: np.array, robot_pose: Pose2D
) -&gt; Tuple[float, int]:
    &#34;&#34;&#34;Calculates minimal distance to global plan using kd-tree-search.

    Args:
        global_plan (np.array): Array containing 2D poses.
        robot_pose (Pose2D): Robot position.

    Returns:
        Tuple[float, int]: Distance to the closes pose and index of the closes pose.
    &#34;&#34;&#34;
    if self.kdtree is None:
        self.kdtree = scipy.spatial.cKDTree(global_plan)

    dist, index = self.kdtree.query([robot_pose.x, robot_pose.y])
    return dist, index</code></pre>
</details>
</dd>
<dt id="rl_agent.utils.reward.RewardCalculator.get_reward"><code class="name flex">
<span>def <span class="ident">get_reward</span></span>(<span>self, laser_scan: numpy.ndarray, goal_in_robot_frame: Tuple[float, float], *args, **kwargs) ‑> Tuple[float, Dict[str, Union[str, int, bool]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns reward and info to the gym environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>laser_scan</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>2D laser scan data.</dd>
<dt><strong><code>goal_in_robot_frame</code></strong> :&ensp;<code>Tuple[float, float]</code></dt>
<dd>Position (rho, theta) of the goal in the robot frame (polar coordinate).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, Dict[str, Union[str, int, bool]]]</code></dt>
<dd>Tuple of calculated rewards for the current step,
and the reward information dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reward(
    self,
    laser_scan: np.ndarray,
    goal_in_robot_frame: Tuple[float, float],
    *args,
    **kwargs
) -&gt; Tuple[float, Dict[str, Union[str, int, bool]]]:
    &#34;&#34;&#34;Returns reward and info to the gym environment.

    Args:
        laser_scan (np.ndarray): 2D laser scan data.
        goal_in_robot_frame (Tuple[float, float]): Position (rho, theta) of the goal in the robot frame (polar coordinate).

    Returns:
        Tuple[float, Dict[str, Union[str, int, bool]]]: Tuple of calculated rewards for the current step, \
            and the reward information dictionary.
    &#34;&#34;&#34;
    self._reset()
    self.cal_func(self, laser_scan, goal_in_robot_frame, *args, **kwargs)
    return self.curr_reward, self.info</code></pre>
</details>
</dd>
<dt id="rl_agent.utils.reward.RewardCalculator.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Resets variables related to the episode.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    &#34;&#34;&#34;Resets variables related to the episode.&#34;&#34;&#34;
    self.last_goal_dist = None
    self.last_dist_to_path = None
    self.last_action = None
    self.kdtree = None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rl_agent.utils" href="index.html">rl_agent.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rl_agent.utils.reward.RewardCalculator" href="#rl_agent.utils.reward.RewardCalculator">RewardCalculator</a></code></h4>
<ul class="">
<li><code><a title="rl_agent.utils.reward.RewardCalculator.get_min_dist2global_kdtree" href="#rl_agent.utils.reward.RewardCalculator.get_min_dist2global_kdtree">get_min_dist2global_kdtree</a></code></li>
<li><code><a title="rl_agent.utils.reward.RewardCalculator.get_reward" href="#rl_agent.utils.reward.RewardCalculator.get_reward">get_reward</a></code></li>
<li><code><a title="rl_agent.utils.reward.RewardCalculator.reset" href="#rl_agent.utils.reward.RewardCalculator.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>