<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>rl_agent.base_agent_wrapper API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rl_agent.base_agent_wrapper</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import ABC, abstractmethod
from typing import Tuple

import json
import numpy as np
import os
import rospy
import rospkg
import yaml

from gym import spaces

from geometry_msgs.msg import Twist

from rl_agent.utils.observation_collector import ObservationCollector
from rl_agent.utils.reward import RewardCalculator


ROOT_ROBOT_PATH = os.path.join(
    rospkg.RosPack().get_path(&#34;simulator_setup&#34;), &#34;robot&#34;
)
DEFAULT_ACTION_SPACE = os.path.join(
    rospkg.RosPack().get_path(&#34;arena_local_planner_drl&#34;),
    &#34;configs&#34;,
    &#34;default_settings.yaml&#34;,
)
DEFAULT_HYPERPARAMETER = os.path.join(
    rospkg.RosPack().get_path(&#34;arena_local_planner_drl&#34;),
    &#34;configs&#34;,
    &#34;hyperparameters&#34;,
    &#34;default.json&#34;,
)
DEFAULT_NUM_LASER_BEAMS, DEFAULT_LASER_RANGE = 360, 3.5
GOAL_RADIUS = 0.5


class BaseDRLAgent(ABC):
    def __init__(
        self,
        ns: str = None,
        robot_name: str = None,
        hyperparameter_path: str = DEFAULT_HYPERPARAMETER,
        action_space_path: str = DEFAULT_ACTION_SPACE,
        robot_safe_dist: float = 0.5,
        *args,
        **kwargs,
    ) -&gt; None:
        &#34;&#34;&#34;Base agent class for an DRL agent.

        Args:
            ns (str, optional): Agent name (directory has to be of the same name). Defaults to None.
            robot_name (str, optional): Robot specific ROS namespace extension. Defaults to None.
            hyperparameter_path (str, optional): Path to json file containing defined hyperparameters. \
                Defaults to DEFAULT_HYPERPARAMETER.
            action_space_path (str, optional): Path to yaml file containing action space settings.\
                Defaults to DEFAULT_ACTION_SPACE.
            robot_safe_dist (float, optional): Robots&#39; safe distance in meters. Defaults to 0.5.
        &#34;&#34;&#34;
        self._is_train_mode = rospy.get_param(&#34;/train_mode&#34;)

        self._ns = &#34;&#34; if ns is None or ns == &#34;&#34; else ns + &#34;/&#34;
        self._ns_robot = (
            self._ns if robot_name is None else self._ns + robot_name + &#34;/&#34;
        )
        self._robot_sim_ns, self._safe_dist = robot_name, robot_safe_dist
        # print(robot_name)

        self.load_hyperparameters(path=hyperparameter_path)
        robot_setting_path = os.path.join(
            ROOT_ROBOT_PATH, self.robot_config_name + &#34;.model.yaml&#34;
        )
        self.read_setting_files(robot_setting_path, action_space_path)
        self.setup_action_space()
        self.setup_reward_calculator()

        self.observation_collector = ObservationCollector(
            self._ns_robot, self._num_laser_beams, self._laser_range
        )

        # for time controlling in train mode
        self._action_frequency = 1 / rospy.get_param(&#34;/robot_action_rate&#34;)

        # internal state for doneness
        self.done = False

        if self._is_train_mode:
            # w/o action publisher node
            self._action_pub = rospy.Publisher(
                f&#34;{self._ns_robot}cmd_vel&#34;, Twist, queue_size=1
            )
        else:
            # w/ action publisher node
            # (controls action rate being published on &#39;../cmd_vel&#39;)
            self._action_pub = rospy.Publisher(
                f&#34;{self._ns_robot}cmd_vel_pub&#34;, Twist, queue_size=1
            )

    @abstractmethod
    def setup_agent(self) -&gt; None:
        &#34;&#34;&#34;Sets up the new agent / loads a pretrained one.

        Raises:
            NotImplementedError: Abstract method.
        &#34;&#34;&#34;
        raise NotImplementedError

    def load_hyperparameters(self, path: str) -&gt; None:
        &#34;&#34;&#34;Loads the hyperparameters from a json file.

        Args:
            path (str): Path to the json file.
        &#34;&#34;&#34;
        assert os.path.isfile(
            path
        ), f&#34;Hyperparameters file cannot be found at {path}!&#34;

        with open(path, &#34;r&#34;) as file:
            hyperparams = json.load(file)

        self._agent_params = hyperparams
        self._get_robot_name_from_params()

    def read_setting_files(
        self, robot_setting_yaml: str, action_space_yaml: str
    ) -&gt; None:
        &#34;&#34;&#34;Retrieves the robot radius (in &#39;self._robot_radius&#39;), \
            laser scan range (in &#39;self._laser_range&#39;) and \
            the action space from respective yaml file.

        Args:
            robot_setting_yaml (str): Yaml file containing the robot specific settings. 
            action_space_yaml (str): Yaml file containing the action space configuration. 
        &#34;&#34;&#34;
        self._num_laser_beams = None
        self._laser_range = None

        with open(robot_setting_yaml, &#34;r&#34;) as fd:
            robot_data = yaml.safe_load(fd)

            # get robot radius
            for body in robot_data[&#34;bodies&#34;]:
                if body[&#34;name&#34;] == &#34;base_footprint&#34;:
                    for footprint in body[&#34;footprints&#34;]:
                        if footprint[&#34;type&#34;] == &#34;circle&#34;:
                            self._robot_radius = (
                                footprint.setdefault(&#34;radius&#34;, 0.3) * 1.05
                            )
                        if footprint[&#34;radius&#34;]:
                            self._robot_radius = footprint[&#34;radius&#34;] * 1.05

            # get laser related information
            for plugin in robot_data[&#34;plugins&#34;]:
                if plugin[&#34;type&#34;] == &#34;Laser&#34;:
                    laser_angle_min = plugin[&#34;angle&#34;][&#34;min&#34;]
                    laser_angle_max = plugin[&#34;angle&#34;][&#34;max&#34;]
                    laser_angle_increment = plugin[&#34;angle&#34;][&#34;increment&#34;]
                    self._num_laser_beams = int(
                        round(
                            (laser_angle_max - laser_angle_min)
                            / laser_angle_increment
                        )
                        + 1
                    )
                    self._laser_range = plugin[&#34;range&#34;]

        if self._num_laser_beams is None:
            self._num_laser_beams = DEFAULT_NUM_LASER_BEAMS
            print(
                f&#34;{self._robot_sim_ns}:&#34;
                &#34;Wasn&#39;t able to read the number of laser beams.&#34;
                &#34;Set to default: {DEFAULT_NUM_LASER_BEAMS}&#34;
            )
        if self._laser_range is None:
            self._laser_range = DEFAULT_LASER_RANGE
            print(
                f&#34;{self._robot_sim_ns}:&#34;
                &#34;Wasn&#39;t able to read the laser range.&#34;
                &#34;Set to default: {DEFAULT_LASER_RANGE}&#34;
            )

        with open(action_space_yaml, &#34;r&#34;) as fd:
            setting_data = yaml.safe_load(fd)

            self._discrete_actions = setting_data[&#34;robot&#34;][&#34;discrete_actions&#34;]
            self._cont_actions = {
                &#34;linear_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                    &#34;linear_range&#34;
                ],
                &#34;angular_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                    &#34;angular_range&#34;
                ],
            }

    def _get_robot_name_from_params(self):
        &#34;&#34;&#34;Retrives the agent-specific robot name from the dictionary loaded\
            from respective &#39;hyperparameter.json&#39;.    
        &#34;&#34;&#34;
        assert self._agent_params and self._agent_params[&#34;robot&#34;]
        self.robot_config_name = self._agent_params[&#34;robot&#34;]

    def setup_action_space(self) -&gt; None:
        &#34;&#34;&#34;Sets up the action space. (spaces.Box)&#34;&#34;&#34;
        assert self._discrete_actions or self._cont_actions
        assert (
            self._agent_params and &#34;discrete_action_space&#34; in self._agent_params
        )

        self._action_space = (
            spaces.Discrete(len(self._discrete_actions))
            if self._agent_params[&#34;discrete_action_space&#34;]
            else spaces.Box(
                low=np.array(
                    [
                        self._cont_actions[&#34;linear_range&#34;][0],
                        self._cont_actions[&#34;angular_range&#34;][0],
                    ]
                ),
                high=np.array(
                    [
                        self._cont_actions[&#34;linear_range&#34;][1],
                        self._cont_actions[&#34;angular_range&#34;][1],
                    ]
                ),
                dtype=np.float,
            )
        )

    def setup_reward_calculator(self) -&gt; None:
        &#34;&#34;&#34;Sets up the reward calculator.&#34;&#34;&#34;
        assert self._agent_params and &#34;reward_fnc&#34; in self._agent_params
        self.reward_calculator = RewardCalculator(
            robot_radius=self._robot_radius,
            safe_dist=self._safe_dist,
            goal_radius=GOAL_RADIUS,
            rule=self._agent_params[&#34;reward_fnc&#34;],
            extended_eval=False,
        )

    @property
    def action_space(self) -&gt; spaces.Box:
        &#34;&#34;&#34;Returns the DRL agent&#39;s action space.

        Returns:
            spaces.Box: Agent&#39;s action space
        &#34;&#34;&#34;
        return self._action_space

    @property
    def observation_space(self) -&gt; spaces.Box:
        &#34;&#34;&#34;Returns the DRL agent&#39;s observation space.

        Returns:
            spaces.Box: Agent&#39;s observation space
        &#34;&#34;&#34;
        return self.observation_collector.observation_space

    def get_observations(self) -&gt; Tuple[np.ndarray, dict]:
        &#34;&#34;&#34;Retrieves the latest synchronized observation.

        Returns:
            Tuple[np.ndarray, dict]: 
                Tuple, where first entry depicts the observation data concatenated \
                into one array. Second entry represents the observation dictionary.
        &#34;&#34;&#34;
        merged_obs, obs_dict = self.observation_collector.get_observations()
        if self._agent_params[&#34;normalize&#34;]:
            merged_obs = self.normalize_observations(merged_obs)
        return merged_obs, obs_dict

    def normalize_observations(self, merged_obs: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Normalizes the observations with the loaded VecNormalize object.

        Note:
            VecNormalize object from Stable-Baselines3 is agent specific\
            and integral part in order to map right actions.\

        Args:
            merged_obs (np.ndarray): Observation data concatenated into one array.

        Returns:
            np.ndarray: Normalized observations array.
        &#34;&#34;&#34;
        assert self._agent_params[&#34;normalize&#34;] and hasattr(
            self, &#34;_obs_norm_func&#34;
        )
        return self._obs_norm_func(merged_obs)

    def get_action(
        self, obs: np.ndarray, obs_normalized: bool = False
    ) -&gt; np.ndarray:
        &#34;&#34;&#34;Infers an action based on the given observation.

        Args:
            obs (np.ndarray): Merged observation array.
            obs_normalized (bool): Whether the obs array is already normalized. \
                Defaults to False.

        Returns:
            np.ndarray:
                Action in [linear velocity, angular velocity]
        &#34;&#34;&#34;
        assert self._agent, &#34;Agent model not initialized!&#34;
        if self._agent_params[&#34;normalize&#34;] and not obs_normalized:
            obs = self.normalize_observations(obs)
        action = self._agent.predict(obs, deterministic=True)[0]

        if self._agent_params[&#34;discrete_action_space&#34;]:
            action = self._get_disc_action(action)
        else:
            # clip action
            action = np.maximum(
                np.minimum(self._action_space.high, action),
                self._action_space.low,
            )
        return action

    def get_reward(self, action: np.ndarray, obs_dict: dict) -&gt; float:
        &#34;&#34;&#34;Calculates the reward based on the parsed observation

        Args:
            action (np.ndarray): Velocity commands of the agent in [linear velocity, angular velocity].
            obs_dict (dict): Observation dictionary where each key makes up a different \
                kind of information about the environment.
        Returns:
            float: Reward amount
        &#34;&#34;&#34;
        reward, reward_info = self.reward_calculator.get_reward(
            action=action, **obs_dict
        )
        self.done = reward_info[&#34;is_done&#34;]
        return reward, reward_info

    def publish_action(self, action: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Publishes an action on &#39;self._action_pub&#39; (ROS topic).

        Args:
            action (np.ndarray): Action in [linear velocity, angular velocity]
        &#34;&#34;&#34;
        if not action:
            action = [0, 0]
        action_msg = Twist()
        action_msg.linear.x = action[0]
        action_msg.angular.z = action[1]
        self._action_pub.publish(action_msg)

    def _get_disc_action(self, action: int) -&gt; np.ndarray:
        &#34;&#34;&#34;Returns defined velocity commands for parsed action index.\
            (Discrete action space)

        Args:
            action (int): Index of the desired action.

        Returns:
            np.ndarray: Velocity commands corresponding to the index.
        &#34;&#34;&#34;
        return np.array(
            [
                self._discrete_actions[action][&#34;linear&#34;],
                self._discrete_actions[action][&#34;angular&#34;],
            ]
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent"><code class="flex name class">
<span>class <span class="ident">BaseDRLAgent</span></span>
<span>(</span><span>ns: str = None, robot_name: str = None, hyperparameter_path: str = '/home/tuananhroman/catkin_ws/src/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/configs/hyperparameters/default.json', action_space_path: str = '/home/tuananhroman/catkin_ws/src/arena-rosnav/arena_navigation/arena_local_planner/learning_based/arena_local_planner_drl/configs/default_settings.yaml', robot_safe_dist: float = 0.5, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Base agent class for an DRL agent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ns</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Agent name (directory has to be of the same name). Defaults to None.</dd>
<dt><strong><code>robot_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Robot specific ROS namespace extension. Defaults to None.</dd>
<dt><strong><code>hyperparameter_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to json file containing defined hyperparameters.
Defaults to DEFAULT_HYPERPARAMETER.</dd>
<dt><strong><code>action_space_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Path to yaml file containing action space settings.
Defaults to DEFAULT_ACTION_SPACE.</dd>
<dt><strong><code>robot_safe_dist</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Robots' safe distance in meters. Defaults to 0.5.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDRLAgent(ABC):
    def __init__(
        self,
        ns: str = None,
        robot_name: str = None,
        hyperparameter_path: str = DEFAULT_HYPERPARAMETER,
        action_space_path: str = DEFAULT_ACTION_SPACE,
        robot_safe_dist: float = 0.5,
        *args,
        **kwargs,
    ) -&gt; None:
        &#34;&#34;&#34;Base agent class for an DRL agent.

        Args:
            ns (str, optional): Agent name (directory has to be of the same name). Defaults to None.
            robot_name (str, optional): Robot specific ROS namespace extension. Defaults to None.
            hyperparameter_path (str, optional): Path to json file containing defined hyperparameters. \
                Defaults to DEFAULT_HYPERPARAMETER.
            action_space_path (str, optional): Path to yaml file containing action space settings.\
                Defaults to DEFAULT_ACTION_SPACE.
            robot_safe_dist (float, optional): Robots&#39; safe distance in meters. Defaults to 0.5.
        &#34;&#34;&#34;
        self._is_train_mode = rospy.get_param(&#34;/train_mode&#34;)

        self._ns = &#34;&#34; if ns is None or ns == &#34;&#34; else ns + &#34;/&#34;
        self._ns_robot = (
            self._ns if robot_name is None else self._ns + robot_name + &#34;/&#34;
        )
        self._robot_sim_ns, self._safe_dist = robot_name, robot_safe_dist
        # print(robot_name)

        self.load_hyperparameters(path=hyperparameter_path)
        robot_setting_path = os.path.join(
            ROOT_ROBOT_PATH, self.robot_config_name + &#34;.model.yaml&#34;
        )
        self.read_setting_files(robot_setting_path, action_space_path)
        self.setup_action_space()
        self.setup_reward_calculator()

        self.observation_collector = ObservationCollector(
            self._ns_robot, self._num_laser_beams, self._laser_range
        )

        # for time controlling in train mode
        self._action_frequency = 1 / rospy.get_param(&#34;/robot_action_rate&#34;)

        # internal state for doneness
        self.done = False

        if self._is_train_mode:
            # w/o action publisher node
            self._action_pub = rospy.Publisher(
                f&#34;{self._ns_robot}cmd_vel&#34;, Twist, queue_size=1
            )
        else:
            # w/ action publisher node
            # (controls action rate being published on &#39;../cmd_vel&#39;)
            self._action_pub = rospy.Publisher(
                f&#34;{self._ns_robot}cmd_vel_pub&#34;, Twist, queue_size=1
            )

    @abstractmethod
    def setup_agent(self) -&gt; None:
        &#34;&#34;&#34;Sets up the new agent / loads a pretrained one.

        Raises:
            NotImplementedError: Abstract method.
        &#34;&#34;&#34;
        raise NotImplementedError

    def load_hyperparameters(self, path: str) -&gt; None:
        &#34;&#34;&#34;Loads the hyperparameters from a json file.

        Args:
            path (str): Path to the json file.
        &#34;&#34;&#34;
        assert os.path.isfile(
            path
        ), f&#34;Hyperparameters file cannot be found at {path}!&#34;

        with open(path, &#34;r&#34;) as file:
            hyperparams = json.load(file)

        self._agent_params = hyperparams
        self._get_robot_name_from_params()

    def read_setting_files(
        self, robot_setting_yaml: str, action_space_yaml: str
    ) -&gt; None:
        &#34;&#34;&#34;Retrieves the robot radius (in &#39;self._robot_radius&#39;), \
            laser scan range (in &#39;self._laser_range&#39;) and \
            the action space from respective yaml file.

        Args:
            robot_setting_yaml (str): Yaml file containing the robot specific settings. 
            action_space_yaml (str): Yaml file containing the action space configuration. 
        &#34;&#34;&#34;
        self._num_laser_beams = None
        self._laser_range = None

        with open(robot_setting_yaml, &#34;r&#34;) as fd:
            robot_data = yaml.safe_load(fd)

            # get robot radius
            for body in robot_data[&#34;bodies&#34;]:
                if body[&#34;name&#34;] == &#34;base_footprint&#34;:
                    for footprint in body[&#34;footprints&#34;]:
                        if footprint[&#34;type&#34;] == &#34;circle&#34;:
                            self._robot_radius = (
                                footprint.setdefault(&#34;radius&#34;, 0.3) * 1.05
                            )
                        if footprint[&#34;radius&#34;]:
                            self._robot_radius = footprint[&#34;radius&#34;] * 1.05

            # get laser related information
            for plugin in robot_data[&#34;plugins&#34;]:
                if plugin[&#34;type&#34;] == &#34;Laser&#34;:
                    laser_angle_min = plugin[&#34;angle&#34;][&#34;min&#34;]
                    laser_angle_max = plugin[&#34;angle&#34;][&#34;max&#34;]
                    laser_angle_increment = plugin[&#34;angle&#34;][&#34;increment&#34;]
                    self._num_laser_beams = int(
                        round(
                            (laser_angle_max - laser_angle_min)
                            / laser_angle_increment
                        )
                        + 1
                    )
                    self._laser_range = plugin[&#34;range&#34;]

        if self._num_laser_beams is None:
            self._num_laser_beams = DEFAULT_NUM_LASER_BEAMS
            print(
                f&#34;{self._robot_sim_ns}:&#34;
                &#34;Wasn&#39;t able to read the number of laser beams.&#34;
                &#34;Set to default: {DEFAULT_NUM_LASER_BEAMS}&#34;
            )
        if self._laser_range is None:
            self._laser_range = DEFAULT_LASER_RANGE
            print(
                f&#34;{self._robot_sim_ns}:&#34;
                &#34;Wasn&#39;t able to read the laser range.&#34;
                &#34;Set to default: {DEFAULT_LASER_RANGE}&#34;
            )

        with open(action_space_yaml, &#34;r&#34;) as fd:
            setting_data = yaml.safe_load(fd)

            self._discrete_actions = setting_data[&#34;robot&#34;][&#34;discrete_actions&#34;]
            self._cont_actions = {
                &#34;linear_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                    &#34;linear_range&#34;
                ],
                &#34;angular_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                    &#34;angular_range&#34;
                ],
            }

    def _get_robot_name_from_params(self):
        &#34;&#34;&#34;Retrives the agent-specific robot name from the dictionary loaded\
            from respective &#39;hyperparameter.json&#39;.    
        &#34;&#34;&#34;
        assert self._agent_params and self._agent_params[&#34;robot&#34;]
        self.robot_config_name = self._agent_params[&#34;robot&#34;]

    def setup_action_space(self) -&gt; None:
        &#34;&#34;&#34;Sets up the action space. (spaces.Box)&#34;&#34;&#34;
        assert self._discrete_actions or self._cont_actions
        assert (
            self._agent_params and &#34;discrete_action_space&#34; in self._agent_params
        )

        self._action_space = (
            spaces.Discrete(len(self._discrete_actions))
            if self._agent_params[&#34;discrete_action_space&#34;]
            else spaces.Box(
                low=np.array(
                    [
                        self._cont_actions[&#34;linear_range&#34;][0],
                        self._cont_actions[&#34;angular_range&#34;][0],
                    ]
                ),
                high=np.array(
                    [
                        self._cont_actions[&#34;linear_range&#34;][1],
                        self._cont_actions[&#34;angular_range&#34;][1],
                    ]
                ),
                dtype=np.float,
            )
        )

    def setup_reward_calculator(self) -&gt; None:
        &#34;&#34;&#34;Sets up the reward calculator.&#34;&#34;&#34;
        assert self._agent_params and &#34;reward_fnc&#34; in self._agent_params
        self.reward_calculator = RewardCalculator(
            robot_radius=self._robot_radius,
            safe_dist=self._safe_dist,
            goal_radius=GOAL_RADIUS,
            rule=self._agent_params[&#34;reward_fnc&#34;],
            extended_eval=False,
        )

    @property
    def action_space(self) -&gt; spaces.Box:
        &#34;&#34;&#34;Returns the DRL agent&#39;s action space.

        Returns:
            spaces.Box: Agent&#39;s action space
        &#34;&#34;&#34;
        return self._action_space

    @property
    def observation_space(self) -&gt; spaces.Box:
        &#34;&#34;&#34;Returns the DRL agent&#39;s observation space.

        Returns:
            spaces.Box: Agent&#39;s observation space
        &#34;&#34;&#34;
        return self.observation_collector.observation_space

    def get_observations(self) -&gt; Tuple[np.ndarray, dict]:
        &#34;&#34;&#34;Retrieves the latest synchronized observation.

        Returns:
            Tuple[np.ndarray, dict]: 
                Tuple, where first entry depicts the observation data concatenated \
                into one array. Second entry represents the observation dictionary.
        &#34;&#34;&#34;
        merged_obs, obs_dict = self.observation_collector.get_observations()
        if self._agent_params[&#34;normalize&#34;]:
            merged_obs = self.normalize_observations(merged_obs)
        return merged_obs, obs_dict

    def normalize_observations(self, merged_obs: np.ndarray) -&gt; np.ndarray:
        &#34;&#34;&#34;Normalizes the observations with the loaded VecNormalize object.

        Note:
            VecNormalize object from Stable-Baselines3 is agent specific\
            and integral part in order to map right actions.\

        Args:
            merged_obs (np.ndarray): Observation data concatenated into one array.

        Returns:
            np.ndarray: Normalized observations array.
        &#34;&#34;&#34;
        assert self._agent_params[&#34;normalize&#34;] and hasattr(
            self, &#34;_obs_norm_func&#34;
        )
        return self._obs_norm_func(merged_obs)

    def get_action(
        self, obs: np.ndarray, obs_normalized: bool = False
    ) -&gt; np.ndarray:
        &#34;&#34;&#34;Infers an action based on the given observation.

        Args:
            obs (np.ndarray): Merged observation array.
            obs_normalized (bool): Whether the obs array is already normalized. \
                Defaults to False.

        Returns:
            np.ndarray:
                Action in [linear velocity, angular velocity]
        &#34;&#34;&#34;
        assert self._agent, &#34;Agent model not initialized!&#34;
        if self._agent_params[&#34;normalize&#34;] and not obs_normalized:
            obs = self.normalize_observations(obs)
        action = self._agent.predict(obs, deterministic=True)[0]

        if self._agent_params[&#34;discrete_action_space&#34;]:
            action = self._get_disc_action(action)
        else:
            # clip action
            action = np.maximum(
                np.minimum(self._action_space.high, action),
                self._action_space.low,
            )
        return action

    def get_reward(self, action: np.ndarray, obs_dict: dict) -&gt; float:
        &#34;&#34;&#34;Calculates the reward based on the parsed observation

        Args:
            action (np.ndarray): Velocity commands of the agent in [linear velocity, angular velocity].
            obs_dict (dict): Observation dictionary where each key makes up a different \
                kind of information about the environment.
        Returns:
            float: Reward amount
        &#34;&#34;&#34;
        reward, reward_info = self.reward_calculator.get_reward(
            action=action, **obs_dict
        )
        self.done = reward_info[&#34;is_done&#34;]
        return reward, reward_info

    def publish_action(self, action: np.ndarray) -&gt; None:
        &#34;&#34;&#34;Publishes an action on &#39;self._action_pub&#39; (ROS topic).

        Args:
            action (np.ndarray): Action in [linear velocity, angular velocity]
        &#34;&#34;&#34;
        if not action:
            action = [0, 0]
        action_msg = Twist()
        action_msg.linear.x = action[0]
        action_msg.angular.z = action[1]
        self._action_pub.publish(action_msg)

    def _get_disc_action(self, action: int) -&gt; np.ndarray:
        &#34;&#34;&#34;Returns defined velocity commands for parsed action index.\
            (Discrete action space)

        Args:
            action (int): Index of the desired action.

        Returns:
            np.ndarray: Velocity commands corresponding to the index.
        &#34;&#34;&#34;
        return np.array(
            [
                self._discrete_actions[action][&#34;linear&#34;],
                self._discrete_actions[action][&#34;angular&#34;],
            ]
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="rl_agent.training_agent_wrapper.TrainingDRLAgent" href="training_agent_wrapper.html#rl_agent.training_agent_wrapper.TrainingDRLAgent">TrainingDRLAgent</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.action_space"><code class="name">var <span class="ident">action_space</span> : gym.spaces.box.Box</code></dt>
<dd>
<div class="desc"><p>Returns the DRL agent's action space.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>spaces.Box</code></dt>
<dd>Agent's action space</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def action_space(self) -&gt; spaces.Box:
    &#34;&#34;&#34;Returns the DRL agent&#39;s action space.

    Returns:
        spaces.Box: Agent&#39;s action space
    &#34;&#34;&#34;
    return self._action_space</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.observation_space"><code class="name">var <span class="ident">observation_space</span> : gym.spaces.box.Box</code></dt>
<dd>
<div class="desc"><p>Returns the DRL agent's observation space.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>spaces.Box</code></dt>
<dd>Agent's observation space</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def observation_space(self) -&gt; spaces.Box:
    &#34;&#34;&#34;Returns the DRL agent&#39;s observation space.

    Returns:
        spaces.Box: Agent&#39;s observation space
    &#34;&#34;&#34;
    return self.observation_collector.observation_space</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.get_action"><code class="name flex">
<span>def <span class="ident">get_action</span></span>(<span>self, obs: numpy.ndarray, obs_normalized: bool = False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Infers an action based on the given observation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Merged observation array.</dd>
<dt><strong><code>obs_normalized</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the obs array is already normalized.
Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>np.ndarray:
Action in [linear velocity, angular velocity]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_action(
    self, obs: np.ndarray, obs_normalized: bool = False
) -&gt; np.ndarray:
    &#34;&#34;&#34;Infers an action based on the given observation.

    Args:
        obs (np.ndarray): Merged observation array.
        obs_normalized (bool): Whether the obs array is already normalized. \
            Defaults to False.

    Returns:
        np.ndarray:
            Action in [linear velocity, angular velocity]
    &#34;&#34;&#34;
    assert self._agent, &#34;Agent model not initialized!&#34;
    if self._agent_params[&#34;normalize&#34;] and not obs_normalized:
        obs = self.normalize_observations(obs)
    action = self._agent.predict(obs, deterministic=True)[0]

    if self._agent_params[&#34;discrete_action_space&#34;]:
        action = self._get_disc_action(action)
    else:
        # clip action
        action = np.maximum(
            np.minimum(self._action_space.high, action),
            self._action_space.low,
        )
    return action</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.get_observations"><code class="name flex">
<span>def <span class="ident">get_observations</span></span>(<span>self) ‑> Tuple[numpy.ndarray, dict]</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the latest synchronized observation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[np.ndarray, dict]</code></dt>
<dd>Tuple, where first entry depicts the observation data concatenated
into one array. Second entry represents the observation dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_observations(self) -&gt; Tuple[np.ndarray, dict]:
    &#34;&#34;&#34;Retrieves the latest synchronized observation.

    Returns:
        Tuple[np.ndarray, dict]: 
            Tuple, where first entry depicts the observation data concatenated \
            into one array. Second entry represents the observation dictionary.
    &#34;&#34;&#34;
    merged_obs, obs_dict = self.observation_collector.get_observations()
    if self._agent_params[&#34;normalize&#34;]:
        merged_obs = self.normalize_observations(merged_obs)
    return merged_obs, obs_dict</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.get_reward"><code class="name flex">
<span>def <span class="ident">get_reward</span></span>(<span>self, action: numpy.ndarray, obs_dict: dict) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the reward based on the parsed observation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Velocity commands of the agent in [linear velocity, angular velocity].</dd>
<dt><strong><code>obs_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>Observation dictionary where each key makes up a different
kind of information about the environment.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Reward amount</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reward(self, action: np.ndarray, obs_dict: dict) -&gt; float:
    &#34;&#34;&#34;Calculates the reward based on the parsed observation

    Args:
        action (np.ndarray): Velocity commands of the agent in [linear velocity, angular velocity].
        obs_dict (dict): Observation dictionary where each key makes up a different \
            kind of information about the environment.
    Returns:
        float: Reward amount
    &#34;&#34;&#34;
    reward, reward_info = self.reward_calculator.get_reward(
        action=action, **obs_dict
    )
    self.done = reward_info[&#34;is_done&#34;]
    return reward, reward_info</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.load_hyperparameters"><code class="name flex">
<span>def <span class="ident">load_hyperparameters</span></span>(<span>self, path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the hyperparameters from a json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the json file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_hyperparameters(self, path: str) -&gt; None:
    &#34;&#34;&#34;Loads the hyperparameters from a json file.

    Args:
        path (str): Path to the json file.
    &#34;&#34;&#34;
    assert os.path.isfile(
        path
    ), f&#34;Hyperparameters file cannot be found at {path}!&#34;

    with open(path, &#34;r&#34;) as file:
        hyperparams = json.load(file)

    self._agent_params = hyperparams
    self._get_robot_name_from_params()</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.normalize_observations"><code class="name flex">
<span>def <span class="ident">normalize_observations</span></span>(<span>self, merged_obs: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes the observations with the loaded VecNormalize object.</p>
<h2 id="note">Note</h2>
<p>VecNormalize object from Stable-Baselines3 is agent specific
and integral part in order to map right actions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>merged_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Observation data concatenated into one array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Normalized observations array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_observations(self, merged_obs: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;Normalizes the observations with the loaded VecNormalize object.

    Note:
        VecNormalize object from Stable-Baselines3 is agent specific\
        and integral part in order to map right actions.\

    Args:
        merged_obs (np.ndarray): Observation data concatenated into one array.

    Returns:
        np.ndarray: Normalized observations array.
    &#34;&#34;&#34;
    assert self._agent_params[&#34;normalize&#34;] and hasattr(
        self, &#34;_obs_norm_func&#34;
    )
    return self._obs_norm_func(merged_obs)</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.publish_action"><code class="name flex">
<span>def <span class="ident">publish_action</span></span>(<span>self, action: numpy.ndarray) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Publishes an action on 'self._action_pub' (ROS topic).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Action in [linear velocity, angular velocity]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def publish_action(self, action: np.ndarray) -&gt; None:
    &#34;&#34;&#34;Publishes an action on &#39;self._action_pub&#39; (ROS topic).

    Args:
        action (np.ndarray): Action in [linear velocity, angular velocity]
    &#34;&#34;&#34;
    if not action:
        action = [0, 0]
    action_msg = Twist()
    action_msg.linear.x = action[0]
    action_msg.angular.z = action[1]
    self._action_pub.publish(action_msg)</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.read_setting_files"><code class="name flex">
<span>def <span class="ident">read_setting_files</span></span>(<span>self, robot_setting_yaml: str, action_space_yaml: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves the robot radius (in 'self._robot_radius'),
laser scan range (in 'self._laser_range') and
the action space from respective yaml file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>robot_setting_yaml</code></strong> :&ensp;<code>str</code></dt>
<dd>Yaml file containing the robot specific settings. </dd>
<dt><strong><code>action_space_yaml</code></strong> :&ensp;<code>str</code></dt>
<dd>Yaml file containing the action space configuration.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_setting_files(
    self, robot_setting_yaml: str, action_space_yaml: str
) -&gt; None:
    &#34;&#34;&#34;Retrieves the robot radius (in &#39;self._robot_radius&#39;), \
        laser scan range (in &#39;self._laser_range&#39;) and \
        the action space from respective yaml file.

    Args:
        robot_setting_yaml (str): Yaml file containing the robot specific settings. 
        action_space_yaml (str): Yaml file containing the action space configuration. 
    &#34;&#34;&#34;
    self._num_laser_beams = None
    self._laser_range = None

    with open(robot_setting_yaml, &#34;r&#34;) as fd:
        robot_data = yaml.safe_load(fd)

        # get robot radius
        for body in robot_data[&#34;bodies&#34;]:
            if body[&#34;name&#34;] == &#34;base_footprint&#34;:
                for footprint in body[&#34;footprints&#34;]:
                    if footprint[&#34;type&#34;] == &#34;circle&#34;:
                        self._robot_radius = (
                            footprint.setdefault(&#34;radius&#34;, 0.3) * 1.05
                        )
                    if footprint[&#34;radius&#34;]:
                        self._robot_radius = footprint[&#34;radius&#34;] * 1.05

        # get laser related information
        for plugin in robot_data[&#34;plugins&#34;]:
            if plugin[&#34;type&#34;] == &#34;Laser&#34;:
                laser_angle_min = plugin[&#34;angle&#34;][&#34;min&#34;]
                laser_angle_max = plugin[&#34;angle&#34;][&#34;max&#34;]
                laser_angle_increment = plugin[&#34;angle&#34;][&#34;increment&#34;]
                self._num_laser_beams = int(
                    round(
                        (laser_angle_max - laser_angle_min)
                        / laser_angle_increment
                    )
                    + 1
                )
                self._laser_range = plugin[&#34;range&#34;]

    if self._num_laser_beams is None:
        self._num_laser_beams = DEFAULT_NUM_LASER_BEAMS
        print(
            f&#34;{self._robot_sim_ns}:&#34;
            &#34;Wasn&#39;t able to read the number of laser beams.&#34;
            &#34;Set to default: {DEFAULT_NUM_LASER_BEAMS}&#34;
        )
    if self._laser_range is None:
        self._laser_range = DEFAULT_LASER_RANGE
        print(
            f&#34;{self._robot_sim_ns}:&#34;
            &#34;Wasn&#39;t able to read the laser range.&#34;
            &#34;Set to default: {DEFAULT_LASER_RANGE}&#34;
        )

    with open(action_space_yaml, &#34;r&#34;) as fd:
        setting_data = yaml.safe_load(fd)

        self._discrete_actions = setting_data[&#34;robot&#34;][&#34;discrete_actions&#34;]
        self._cont_actions = {
            &#34;linear_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                &#34;linear_range&#34;
            ],
            &#34;angular_range&#34;: setting_data[&#34;robot&#34;][&#34;continuous_actions&#34;][
                &#34;angular_range&#34;
            ],
        }</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_action_space"><code class="name flex">
<span>def <span class="ident">setup_action_space</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the action space. (spaces.Box)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_action_space(self) -&gt; None:
    &#34;&#34;&#34;Sets up the action space. (spaces.Box)&#34;&#34;&#34;
    assert self._discrete_actions or self._cont_actions
    assert (
        self._agent_params and &#34;discrete_action_space&#34; in self._agent_params
    )

    self._action_space = (
        spaces.Discrete(len(self._discrete_actions))
        if self._agent_params[&#34;discrete_action_space&#34;]
        else spaces.Box(
            low=np.array(
                [
                    self._cont_actions[&#34;linear_range&#34;][0],
                    self._cont_actions[&#34;angular_range&#34;][0],
                ]
            ),
            high=np.array(
                [
                    self._cont_actions[&#34;linear_range&#34;][1],
                    self._cont_actions[&#34;angular_range&#34;][1],
                ]
            ),
            dtype=np.float,
        )
    )</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_agent"><code class="name flex">
<span>def <span class="ident">setup_agent</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the new agent / loads a pretrained one.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>Abstract method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def setup_agent(self) -&gt; None:
    &#34;&#34;&#34;Sets up the new agent / loads a pretrained one.

    Raises:
        NotImplementedError: Abstract method.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_reward_calculator"><code class="name flex">
<span>def <span class="ident">setup_reward_calculator</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the reward calculator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup_reward_calculator(self) -&gt; None:
    &#34;&#34;&#34;Sets up the reward calculator.&#34;&#34;&#34;
    assert self._agent_params and &#34;reward_fnc&#34; in self._agent_params
    self.reward_calculator = RewardCalculator(
        robot_radius=self._robot_radius,
        safe_dist=self._safe_dist,
        goal_radius=GOAL_RADIUS,
        rule=self._agent_params[&#34;reward_fnc&#34;],
        extended_eval=False,
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rl_agent" href="index.html">rl_agent</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent" href="#rl_agent.base_agent_wrapper.BaseDRLAgent">BaseDRLAgent</a></code></h4>
<ul class="">
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.action_space" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.action_space">action_space</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.get_action" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.get_action">get_action</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.get_observations" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.get_observations">get_observations</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.get_reward" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.get_reward">get_reward</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.load_hyperparameters" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.load_hyperparameters">load_hyperparameters</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.normalize_observations" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.normalize_observations">normalize_observations</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.observation_space" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.observation_space">observation_space</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.publish_action" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.publish_action">publish_action</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.read_setting_files" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.read_setting_files">read_setting_files</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_action_space" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.setup_action_space">setup_action_space</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_agent" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.setup_agent">setup_agent</a></code></li>
<li><code><a title="rl_agent.base_agent_wrapper.BaseDRLAgent.setup_reward_calculator" href="#rl_agent.base_agent_wrapper.BaseDRLAgent.setup_reward_calculator">setup_reward_calculator</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>